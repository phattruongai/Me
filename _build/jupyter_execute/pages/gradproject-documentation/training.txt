# Huấn luyện

from google.colab import drive
drive.mount('/content/drive')

import tensorflow as tf
import keras
import cv2
import numpy as np
import os
import h5py
from matplotlib import pyplot as plt
import time
os.chdir('./drive/My Drive/GG_colab/CDTproject')

tf.__version__

with h5py.File("train_data_100_18-02.h5",'r') as F:
    x_train = np.array(F.get("x_train"))
    y_train = np.array(F.get("y_train"))
    x_test = np.array(F.get("x_test"))
    y_test = np.array(F.get("y_test"))

print(x_train.shape,y_train.shape)
print(x_test.shape,y_test.shape)


from sklearn.model_selection import train_test_split

x_train,x_val,y_train,y_val = train_test_split(x_train,y_train,test_size = 0.12,shuffle = True)
print(x_train.shape,y_train.shape)
print(x_val.shape,y_val.shape)

count = np.zeros((6,),dtype=np.int16)
for i,label in enumerate(y_train):
  num = np.argmax(label)
  count[num] += 1
sumtrain = np.sum(count)
count = count/sumtrain
print(count)

countval = np.zeros((6,),dtype=np.int16)
for i,label in enumerate(y_val):
  num = np.argmax(label)
  countval[num] += 1
sumval = np.sum(countval)
countval = countval/sumval
print(countval)

counttest = np.zeros((6,),dtype=np.int16)
for i,label in enumerate(y_train):
  num = np.argmax(label)
  counttest[num] += 1
sumtest = np.sum(counttest)
counttest = counttest/sumtest
print(counttest)

## Pretrain model

from keras.applications.resnet50 import ResNet50
base_model = ResNet50(weights='imagenet',include_top = False, input_shape = (100,100,3))

#for layer in base_model.layers:
  #layer.trainable = False
x = base_model.output
x = keras.layers.GlobalAvgPool2D()(x)
x = keras.layers.Dropout(rate = 0.5)(x)
x = keras.layers.Dense(6,activation='softmax')(x)
model = keras.models.Model(base_model.input,x)
model.summary()

## Architecture 1 & 2

def Tomato_model(input_shape):
  x_input = keras.layers.Input(input_shape)
  
  x = keras.layers.Conv2D(128,(5,5),padding = 'same')(x_input)
  x = keras.layers.BatchNormalization()(x)
  x = keras.layers.MaxPool2D((2,2))(x)
  x = keras.layers.ReLU()(x)
  
  x = keras.layers.Conv2D(64,(5,5),padding = 'same')(x)
  x = keras.layers.BatchNormalization()(x)
  x = keras.layers.MaxPool2D((2,2))(x)
  x = keras.layers.ReLU()(x)
  
  x = keras.layers.Conv2D(64,(3,3),padding = 'same')(x)
  x = keras.layers.BatchNormalization()(x)
  x = keras.layers.MaxPool2D((2,2))(x)
  x = keras.layers.ReLU()(x)
  
  x = keras.layers.Conv2D(32,(3,3),padding = 'same')(x)
  x = keras.layers.BatchNormalization()(x)
  x = keras.layers.MaxPool2D((2,2))(x)
  x = keras.layers.ReLU()(x)
  
  x = keras.layers.Flatten()(x)
  x = keras.layers.Dense(6,activation = 'softmax')(x)
  
  model = keras.models.Model(inputs = x_input, outputs = x)
  
  return model

model = Tomato_model((100,100,3))
model.summary()

## Architecture 3

def bottleneck_res_block(block_input,factor):
  ###expansion convolution layer
  x = keras.layers.Conv2D(int(factor)*int(block_input.shape[3]),(1,1))(block_input) 
  x = keras.layers.BatchNormalization()(x)
  x = keras.layers.ReLU(max_value = 6)(x)
  
  ###depthwise convolution layer
  x = keras.layers.DepthwiseConv2D((3,3),padding = 'same')(x)
  x = keras.layers.BatchNormalization()(x)
  x = keras.layers.ReLU(max_value = 6)(x)
  
  ###projection convolution layer
  x = keras.layers.Conv2D(int(int(x.shape[3])/int(factor)),(1,1))(x)
  x = keras.layers.BatchNormalization()(x)
  #x = keras.layers.ReLU(max_value = 6)(x)
  
  #Residual connect
  x = keras.layers.Add()([x,block_input])
  x = keras.layers.Dropout(rate = 0.5)(x)
  
  return x

def Tomato_model(input_shape):
  x_input = keras.layers.Input(input_shape)
  
  x = keras.layers.Conv2D(32,(1,1),padding = 'same')(x_input)
  x = keras.layers.BatchNormalization()(x)
  x = keras.layers.MaxPool2D((2,2))(x)
  x = keras.layers.ReLU(max_value = 6)(x)
  x = keras.layers.Dropout(rate = 0.2)(x)
  x = bottleneck_res_block(x,2)
  x = keras.layers.AveragePooling2D((2,2))(x)
  x = bottleneck_res_block(x,2)
  x = keras.layers.GlobalAveragePooling2D()(x)
  x = keras.layers.Dense(6,activation = 'softmax')(x)
  
  model = keras.models.Model(inputs = x_input, outputs = x)
  
  return model

model = Tomato_model((100,100,3))
model.summary()

## Train

from keras.models import load_model
model = load_model("trained_model/size_100_19-01/weights.100.04-0.90.h5")

model.compile(optimizer ='adam', loss = 'categorical_crossentropy',metrics = ['accuracy'])

#filepath="trained_model/size_100_18-02/weights.100.{epoch:02d}-{val_acc:.2f}.h5"
#checkpoint = keras.callbacks.ModelCheckpoint(filepath, monitor='val_acc', verbose=0, mode='auto')
#callbacks_list = [checkpoint]
model.fit(x = x_train, y = y_train, epochs = 300,validation_split=0.2)#,callbacks = callbacks_list)

from scipy.signal import savgol_filter
from numpy import log as ln
from numpy import log10 as log
history = model.history
acc = savgol_filter(history.history['acc'],51,10)
val_acc = savgol_filter(history.history['val_acc'],51,10)
plt.rcParams["figure.figsize"] = (9,6)
plt.plot(acc)
plt.plot(val_acc)
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'valid'], loc='upper left')
plt.show()

# summarize history for loss
plt.plot(history.history['loss'][0:299])
plt.plot(history.history['val_loss'][0:299])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

from tensorflow.keras.models import load_model
mtest = load_model("trained_model/size_100_18-02/weights.100.150-1.00.h5")

mtest.evaluate(x_train[0:1000],y_train[0:1000])


%matplotlib inline
import os
import random
import numpy as np
import json
import matplotlib.pyplot
import pickle
from matplotlib.pyplot import imshow
from PIL import Image
from sklearn.manifold import TSNE


import os
import keras
from keras.preprocessing import image
from keras.applications.imagenet_utils import decode_predictions, preprocess_input
from keras.models import Model

model = keras.applications.VGG16(weights='imagenet', include_top=True)

import numpy as np
import matplotlib.pyplot as plt

def load_image(path):
    img = image.load_img(path, target_size=model.input_shape[1:3])
    x = image.img_to_array(img)
    x = np.expand_dims(x, axis=0)
    x = preprocess_input(x)
    return img, x

img, x = load_image("tsne/loai1/IMG_scale87.jpg")
print("shape of x: ", x.shape)
print("data type: ", x.dtype)
plt.imshow(img)
predictions = model.predict(x)

# print out the 
for _, pred, prob in decode_predictions(predictions)[0]:
    print("predicted %s with probability %0.3f" % (pred, prob))

feat_extractor = Model(inputs=model.input, outputs=model.get_layer("fc2").output)
feat_extractor.summary()

img, x = load_image("tsne/loai1/IMG_scale87.jpg")
feat = feat_extractor.predict(x)

plt.figure(figsize=(16,4))
plt.plot(feat[0])


images_path = 'tsne'
image_extensions = ['.jpg', '.png', '.jpeg']   # case-insensitive (upper/lower doesn't matter)
max_num_images = 10000

images = [os.path.join(dp, f) for dp, dn, filenames in os.walk(images_path) for f in filenames if os.path.splitext(f)[1].lower() in image_extensions]
if max_num_images < len(images):
    images = [images[i] for i in sorted(random.sample(xrange(len(images)), max_num_images))]

print("keeping %d images to analyze" % len(images))

import time
tic = time.clock()


features = []
for i, image_path in enumerate(images):
    if i % 500 == 0:
        toc = time.clock()
        elap = toc-tic;
        print("analyzing image %d / %d. Time: %4.4f seconds." % (i, len(images),elap))
        tic = time.clock()
    img, x = load_image(image_path);
    feat = feat_extractor.predict(x)[0]
    features.append(feat)

print('finished extracting features for %d images' % len(images))

from sklearn.decomposition import PCA

features = np.array(features)
pca = PCA(n_components=50)
pca.fit(features)
pca_features = pca.transform(features)

import random

# grab a random query image
query_image_idx = int(len(images) * random.random())

# let's display the image
img = image.load_img(images[query_image_idx])
plt.imshow(img)

from scipy.spatial import distance

similar_idx = [ distance.cosine(pca_features[query_image_idx], feat) for feat in pca_features ]

idx_closest = sorted(range(len(similar_idx)), key=lambda k: similar_idx[k])[1:6]

thumbs = []
for idx in idx_closest:
    img = image.load_img(images[idx])
    img = img.resize((int(img.width * 100 / img.height), 100))
    thumbs.append(img)

# concatenate the images into a single image
concat_image = np.concatenate([np.asarray(t) for t in thumbs], axis=1)

# show the image
plt.figure(figsize = (16,12))
plt.imshow(concat_image)


def get_closest_images(query_image_idx, num_results=5):
    distances = [ distance.cosine(pca_features[query_image_idx], feat) for feat in pca_features ]
    idx_closest = sorted(range(len(distances)), key=lambda k: distances[k])[1:num_results+1]
    return idx_closest

def get_concatenated_images(indexes, thumb_height):
    thumbs = []
    for idx in indexes:
        img = image.load_img(images[idx])
        img = img.resize((int(img.width * thumb_height / img.height), thumb_height))
        thumbs.append(img)
    concat_image = np.concatenate([np.asarray(t) for t in thumbs], axis=1)
    return concat_image

query_image_idx = int(len(images) * random.random())
idx_closest = get_closest_images(query_image_idx)
query_image = get_concatenated_images([query_image_idx], 300)
results_image = get_concatenated_images(idx_closest, 200)

# display the query image
plt.figure(figsize = (5,5))
plt.imshow(query_image)
plt.title("query image (%d)" % query_image_idx)

# display the resulting images
plt.figure(figsize = (16,12))
plt.imshow(results_image)
plt.title("result images")

query_image_idx = int(len(images) * random.random())
idx_closest = get_closest_images(query_image_idx)
query_image = get_concatenated_images([query_image_idx], 300)
results_image = get_concatenated_images(idx_closest, 200)

# display the query image
plt.figure(figsize = (5,5))
plt.imshow(query_image)
plt.title("query image (%d)" % query_image_idx)

# display the resulting images
plt.figure(figsize = (16,12))
plt.imshow(results_image)
plt.title("result images")

import pickle

pickle.dump([images, pca_features, pca], open('features_tomato.p', 'wb'))

images, pca_features, pca = pickle.load(open('features_tomato.p', 'rb'))

for img, f in list(zip(images, pca_features))[0:5]:
    print("image: %s, features: %0.2f,%0.2f,%0.2f,%0.2f... "%(img, f[0], f[1], f[2], f[3]))

num_images_to_plot = 55

if len(images) > num_images_to_plot:
    sort_order = sorted(random.sample(range(len(images)), num_images_to_plot))
    images = [images[i] for i in sort_order]
    pca_features = [pca_features[i] for i in sort_order]


X = np.array(pca_features)
tsne = TSNE(n_components=2, learning_rate=150, perplexity=30, angle=0.2, verbose=2).fit_transform(X)

tx, ty = tsne[:,0], tsne[:,1]
tx = (tx-np.min(tx)) / (np.max(tx) - np.min(tx))
ty = (ty-np.min(ty)) / (np.max(ty) - np.min(ty))

width = 640
height = 640
max_dim = 100

full_image = Image.new('RGBA', (width, height))
for img, x, y in zip(images, tx, ty):
    tile = Image.open(img)
    rs = max(1, tile.width/max_dim, tile.height/max_dim)
    tile = tile.resize((int(tile.width/rs), int(tile.height/rs)), Image.ANTIALIAS)
    full_image.paste(tile, (int((width-max_dim)*x), int((height-max_dim)*y)), mask=tile.convert('RGBA'))

matplotlib.pyplot.figure(figsize = (16,12))
imshow(full_image)

full_image.save("tSNE.png")

